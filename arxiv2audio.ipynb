{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper to Audio\n",
    "\n",
    "This notebook will take in an HTML file of a paper and parse it into an audio book.\n",
    "TTS is generated through Kokoro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing non-ASCII characters and multiple spaces.\"\"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def process_paragraph(p_tag) -> str:\n",
    "    \"\"\"Process a paragraph tag by removing unwanted elements and cleaning text.\"\"\"\n",
    "    # Remove math and cite tags\n",
    "    for tag in p_tag.find_all(['math', 'cite']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = clean_text(p_tag.get_text(strip=False))\n",
    "    return text\n",
    "\n",
    "def process_abstract(soup) -> Dict[str, str]:\n",
    "    \"\"\"Extract and process the abstract section.\"\"\"\n",
    "    abstract = soup.find('div', class_='ltx_abstract')\n",
    "    if not abstract:\n",
    "        return None\n",
    "    \n",
    "    abstract_paras = []\n",
    "    for p in abstract.find_all('p', class_='ltx_p'):\n",
    "        text = process_paragraph(p)\n",
    "        if text:\n",
    "            abstract_paras.append(text)\n",
    "    \n",
    "    return {\n",
    "        \"title\": \"Abstract\",\n",
    "        \"content\": \"\\n\".join(abstract_paras)\n",
    "    }\n",
    "\n",
    "def process_section(section) -> Dict[str, str]:\n",
    "    \"\"\"Process a single section and return its title and content.\"\"\"\n",
    "    # Extract title\n",
    "    title_tag = section.find('h2', class_='ltx_title_section')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    title = re.sub(r'^\\d+', '', title)\n",
    "    \n",
    "    # Process paragraphs\n",
    "    paragraphs = []\n",
    "    for p in section.find_all('p', class_='ltx_p'):\n",
    "        text = process_paragraph(p)\n",
    "        if text:\n",
    "            paragraphs.append(text)\n",
    "    \n",
    "    return {\n",
    "        \"title\": title.strip(),\n",
    "        \"content\": \"\\n\".join(paragraphs)\n",
    "    }\n",
    "\n",
    "def parse_paper_html(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Main function to parse the paper HTML and return structured content.\"\"\"\n",
    "    # Read and parse HTML\n",
    "    with open(file_path, \"r\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Initialize result list\n",
    "    section_list = []\n",
    "    \n",
    "    # Process abstract\n",
    "    abstract_section = process_abstract(soup)\n",
    "    if abstract_section:\n",
    "        section_list.append(abstract_section)\n",
    "    \n",
    "    # Process main sections\n",
    "    sections = soup.find_all('section', class_='ltx_section')\n",
    "    for section in sections:\n",
    "        section_data = process_section(section)\n",
    "        section_list.append(section_data)\n",
    "    \n",
    "    return section_list\n",
    "\n",
    "file_path = \"/home/iris/wsl_shared/paper_audio_book_generation/src_data/DeepSeek-V3 Technical Report.html\"\n",
    "section_list = parse_paper_html(file_path)\n",
    "\n",
    "# # Print sections for verification\n",
    "# for section in section_list:\n",
    "#     print(f\"\\n=== {section['title']} ===\")\n",
    "#     print(f\"Content length: {len(section['content'])} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list[3][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kokoro.models import build_model\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL = build_model('kokoro/kokoro-v0_19.pth', device)\n",
    "VOICE_NAME = [\n",
    "    'af', # Default voice is a 50-50 mix of Bella & Sarah\n",
    "    'af_bella', 'af_sarah', 'am_adam', 'am_michael',\n",
    "    'bf_emma', 'bf_isabella', 'bm_george', 'bm_lewis',\n",
    "    'af_nicole', 'af_sky',\n",
    "][0]\n",
    "VOICEPACK = torch.load(f'kokoro/voices/{VOICE_NAME}.pt', weights_only=True).to(device)\n",
    "print(f'Loaded voice: {VOICE_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kokoro.kokoro import generate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "# Download the punkt tokenizer if you haven't already\n",
    "nltk.download('punkt')\n",
    "\n",
    "def text_to_audio(text, model, voicepack, voice_name):\n",
    "    \"\"\"Generate audio for a single piece of text\"\"\"\n",
    "    audio, _ = generate(model, text, voicepack, lang=voice_name[0])\n",
    "    return audio\n",
    "\n",
    "def process_sections(section_list, model, voicepack, voice_name):\n",
    "    # Initialize empty list to store audio arrays\n",
    "    audio_segments = []\n",
    "    \n",
    "    for section in section_list:\n",
    "        print(f\"Procssing Section title: {section['title']}\")\n",
    "        # Generate audio for the title\n",
    "        title_audio = text_to_audio(section[\"title\"], model, voicepack, voice_name)\n",
    "        audio_segments.append(title_audio)\n",
    "        \n",
    "        # Add a short pause after the title\n",
    "        silence = np.zeros(12000)  # 1 second silence at 24kHz\n",
    "        audio_segments.append(silence)\n",
    "        \n",
    "        # Split content into sentences\n",
    "        sentences = nltk.sent_tokenize(section[\"content\"])\n",
    "        \n",
    "        # Generate audio for each sentence\n",
    "        for sentence in tqdm(sentences):\n",
    "            # print(sentence)\n",
    "            try:\n",
    "                sentence_audio = text_to_audio(sentence, model, voicepack, voice_name)\n",
    "            except:\n",
    "                # Split sentence by commas\n",
    "                parts = sentence.split(',')\n",
    "                parts = [part.strip() for part in parts if part.strip()]  # Remove empty parts and whitespace\n",
    "                \n",
    "                # Initialize array for combined audio\n",
    "                audio_parts = []\n",
    "                \n",
    "                # Process each part separately\n",
    "                for part in parts:\n",
    "                    part_audio = text_to_audio(part, model, voicepack, voice_name)\n",
    "                    audio_parts.append(part_audio)\n",
    "                \n",
    "                # Concatenate all parts\n",
    "                sentence_audio = np.concatenate(audio_parts)\n",
    "            audio_segments.append(sentence_audio)\n",
    "\n",
    "            \n",
    "            # Add a short pause between sentences\n",
    "            silence = np.zeros(12000)  # 0.5 second silence\n",
    "            audio_segments.append(silence)\n",
    "            \n",
    "        # Add a longer pause between sections\n",
    "        section_silence = np.zeros(24000)  # 2 seconds silence\n",
    "        audio_segments.append(section_silence)\n",
    "    \n",
    "    # Concatenate all audio segments\n",
    "    combined_audio = np.concatenate(audio_segments)\n",
    "    return combined_audio\n",
    "\n",
    "# Main execution\n",
    "def generate_audiobook(section_list, model, voicepack, voice_name, output_path):\n",
    "    # Process all sections and get combined audio\n",
    "    final_audio = process_sections(section_list, model, voicepack, voice_name)\n",
    "    \n",
    "    # Save as WAV file first\n",
    "    wav_path = output_path.replace('.mp3','.wav')\n",
    "    sf.write(wav_path, final_audio, 24000)\n",
    "    \n",
    "    # Convert to MP3 using pydub\n",
    "    from pydub import AudioSegment\n",
    "    audio = AudioSegment.from_wav(wav_path)\n",
    "    audio.export(output_path, format=\"mp3\")\n",
    "    \n",
    "    # Remove temporary WAV file\n",
    "    import os\n",
    "    os.remove(wav_path)\n",
    "    print(f\"Audio book has been saved to {output_path}\")\n",
    "    return final_audio\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Assuming you have MODEL, VOICEPACK, and VOICE_NAME defined as in your notebook\n",
    "output_file = \"audiobook2.mp3\"\n",
    "final_audio = generate_audiobook(section_list, MODEL, VOICEPACK, VOICE_NAME, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Audio\n",
    "\n",
    "# display(Audio(data=final_audio, rate=24000, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to MP4 for YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip\n",
    "import numpy as np\n",
    "\n",
    "image_clip = ImageClip(\"src_data/DeepSeek-V3TechnicalReport.png\")\n",
    "audio_clip = AudioFileClip(\"DeepSeek-V3 Technical Report.mp3\")\n",
    "video = image_clip.set_audio(audio_clip)\n",
    "video = video.set_duration(audio_clip.duration)\n",
    "video.write_videofile(\"DeepSeek-V3_Technical_Report.mp4\", fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
